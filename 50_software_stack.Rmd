---
layout: page
title: "20분만에 끝내는 딥러닝"
subtitle: "딥러닝 개발 환경"
author:
  name: "[한국 R 사용자회](https://r2bit.com/)"
output:
  html_document: 
    theme:
      version: 4
    toc: yes
    toc_depth: 2
    toc_float: true
    highlight: tango
    code_folding: show
    number_section: true
    self_contained: true
urlcolor: blue
linkcolor: bluee
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: sentence
header-includes: 
  - \usepackage{tikz}
  - \usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
  - \tikzstyle{mynode}=[thick,draw=blue,fill=blue!20,circle,minimum size=22]  
---

```{r setup, include=FALSE}
# source("tools/chunk-options.R")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,
                    comment="", digits = 3, tidy = FALSE, 
                    prompt = FALSE, fig.align = 'center')

library(tidyverse)
```


# 딥러닝 개발 환경

딥러닝 개발환경은 결국 Nvidia GPU 그래픽 카드를 구매하여 마더보드에 장착을 한 후에 이를 구동하기 위한 1) 드라이버 2) C 컴파일러 3) CUDA 4) 딥러닝 프레임워크를 순차적으로 설치하여 딥러닝 응용프로그램 개발에 적용하는 것으로 간략히 정리할 수 있다.

구글 Colab을 사용하는 경우 상기 1~4번 과정이 이미 클라우드 상에 구현되어 있으니 신경을 쓰지 않아도 되지만 극단적으로 연구실/집/회사에 자체 딥러닝 서버 혹은 워크스테이션을 구축한 경우 상기 1~4번 뿐만 아니라 전기세(?)도 신경을 써서 관리해야 한다.

![](assets/tools/deep-learning-stacks.png)

- [Techzizou (Aug 25, 2021), "Install CUDA and CUDNN on Windows & Linux", Published in Geek Culture](https://medium.com/geekculture/install-cuda-and-cudnn-on-windows-linux-52d1501a8805)

# 소프트웨어 스택

## 드라이버 {.tabset}

드라이버는 하드웨어와 운영체제를 연결해주는 소프트웨어다. 예를 들어 GPU 그래픽 카드로 Nvidia Geforce GTX 1050 그래픽 카드 마더보드에 장착했다고 가정하면 운영체제(윈도우 10)에서 인식하여 활용할 수 있도록 드라이버 소프트웨어를 설치해줘야 한다.

오래된 GPU는 GPU 드라이버 지원이 중지될 것이 확실하기 때문에 오래된 GPU를 계속해서 사용하고자 하는 경우 GPU 세대별 지원 드라이버 버전을 확인하고 가능하면 최신 GPU 드라이버 버전을 유지한다.

### 드라이버 다운로드

구입한 Nvidia Geforce 그래픽 카드에 매칭되는 드라이버를 [GEFORCE® 드라이버 다운로드](https://www.nvidia.com/ko-kr/geforce/drivers/)에서 선택하여 설치한다.

![](assets/hardware/gpu_driver_download.jpg)

### 윈도우즈 10 설치 장착모습

![](assets/hardware/GPU_driver.jpg)

### Nvidia 그래픽카드 확인

```{bash, eval = FALSE}
$ nvidia-smi
Sat Jul  9 19:41:41 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.89       Driver Version: 460.89       CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 1050   WDDM  | 00000000:01:00.0  On |                  N/A |
| 20%   35C    P5    N/A /  75W |   1836MiB /  2048MiB |      2%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|

```



## Visual Studio 설치

[Visual Studio](https://visualstudio.microsoft.com/ko/downloads/)를 다운로드 받는 이유는 C 컴파일러를 설치하기 위해 필요한 것이다. Visual Studio IDE를 통해 필요한 개발에 필요한 언어 구성요소를 설치할 수 있다. 예를 들어 .NET, Node.js, 파이썬 등... 하지만 딥러닝을 위해 꼭 필요한 것은 C/C++ 개발 구성요소라 필히 설치하고 너무 낮은 버전 Visual Studio 2015 와 같은 너무 오래된 Visual Studio 버전은 업그레이드 하는 것이 필요하다.

![](assets/hardware/vs-c-compiler.jpg)

## CUDA 설치 {.tabset}

위키백과에 따르면 **CUDA("Compute Unified Device Architecture", 쿠다)**는 그래픽 처리 장치에서 수행하는 알고리즘을 C 프로그래밍 언어를 비롯한 산업 표준 언어를 사용하여 작성할 수 있도록 하는 GPGPU ('GPU의 범용 연산', General-Purpose computing on Graphics Processing Units) 기술이다. CUDA는 엔비디아가 개발해오고 있으며 이 아키텍처를 사용하려면 엔비디아 GPU와 특별한 스트림 처리 드라이버가 필요하다.

CUDA는 응용프로그램을 실행하는 부분과 응용프로그램 개발을 지원하는 툴킷으로 구성된다. GPU 하드웨어에 맞춰 CUDA 버전을 맞춰줘야 하드웨어 성능을 최상으로 유지시킬 수 있다.

![](assets/hardware/CUDA_Compatibility.jpg)

### 설치할 CUDA 버전 검색

CUDA 모든 버전이 Nvidia Geforce 그래픽 카드에 맞지 않기 때문에 예를 들어 GTX 1050을 소유하고 있다면 GTX 1050 제품이 속한 아키텍처가 "파스칼(Pascal)"  이기 때문에 CUDA 8 이후 버전을 [CUDA Toolkit Archive](https://developer.nvidia.com/cuda-toolkit-archive) 에서 찾아 GTX 1050을 가장 잘 지원하는 하드웨어 CUDA 버전을 다운로드해서 설치한다.

[Arnon Shimoni (27/10/2020), "Matching CUDA arch and CUDA gencode for various NVIDIA architectures"](https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/)

GTX 1050에 적합한 CUDA 버전으로 8버전이후 가장 최신 버진이 아닌 11.2를 선택한다. ~~가장 최신 버전을 다운로드 받아 설치하면 안 됩니다~~

![](assets/hardware/gtx_1050_cuda_driver.jpg)

### CUDA 버전 확인

```{bash, eval = FALSE}
$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Mon_Nov_30_19:15:10_Pacific_Standard_Time_2020
Cuda compilation tools, release 11.2, V11.2.67
Build cuda_11.2.r11.2/compiler.29373293_0
```


## cuDNN 설치 {.tabset}

GTX 1050 그래픽 카드 CUDA 버전을 확정(11.2) 했기 때문에 cuDNN 딥러닝 GPU 가속 라이브러리를 [cuDNN Archive](https://developer.nvidia.com/rdp/cudnn-archive) 에서 다운로드 받아 복사하여 붙여넣기를 한다.

### cuDNN 다운로드

`cudnn-windows-x86_64-8.4.0.27_cuda11.6-archive.zip` 파일명을 갖기 때문에 압축을 풀어 `bin\`, `include\`, `lib\` 폴더에 담긴 모든 파일을 `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\` 폴더에 다음과 같이 복사하여 붙여넣기한다.

![](assets/hardware/cuDNN.jpg)

### 복사하여 붙여넣기

![](assets/hardware/cuDNN-copy-paste.png)

이미지 출처: [5. Installing cuDNN](https://medium.com/analytics-vidhya/installing-cuda-and-cudnn-on-windows-d44b8e9876b5)

## 환경변수 등록

`C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\` 폴더에 담긴 CUDA 정보를 윈도우즈 환경으로 등록한다.

- CUDA_PATH &rarr; C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2
- CUDA_PATH_V11_2 &rarr; C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2

![](assets/hardware/cuda_env.png)

## 딥러닝 프레임워크 설치

현 시점에서 가장 인기있는 딥러닝 프레임워크는 텐서플로우와 파이토치다. 

먼저 CUDA (11.2), cuDNN (8.1)에 해당되는 파이썬 버전과 tensorflow 버전은 `tensorflow_gpu-2.7.0`으로 확인되어 이를 설치한다.

<div class = "row">
  <div class = "col-md-6">
**tensorflow**

```{bash, eval = FALSE}
pip3 uninstall tensorflow
pip3 install 'tensorflow-gpu==2.7.0'
```

  </div>
  <div class = "col-md-6">
**pytorch**

```{bash, eval = FALSE}
conda install pytorch torchvision cudatoolkit=11.2 -c pytorch
```

  </div>
</div>


- [윈도우즈](https://www.tensorflow.org/install/source_windows#tested_build_configurations)
- [맥/리눅스](https://www.tensorflow.org/install/source#tested_build_configurations)


# 헬로우 월드 {.tabset}

## 텐서플로우


```{python tensorflow-gpu}
import tensorflow as tf

print(tf.config.list_physical_devices('GPU'))

print(tf.test.is_built_with_cuda)

print(tf.test.gpu_device_name())

print(tf.config.get_visible_devices())
```

## 파이토치

```{python pytorch-gpu, eval = FALSE}
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
print('Using device:', device)
# Using device: cuda

#Additional Info when using cuda
if device.type == 'cuda':
    print(torch.cuda.get_device_name(0))
    print('Memory Usage:')
    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')
    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')
# GeForce GTX 1050
# Memory Usage:
# Allocated: 0.0 GB
# Cached:    0.0 GB

```


# 성능 비교

GPU를 사용해서 딥러닝 모형을 만들면 CPU 대비 얼마나 효과가 있을까? 이 문제에 답을 구하기 위해 MNIST 데이터셋을 Keras로 작성된 코드를 다음 하드웨어를 갖춘 컴퓨터로 확인해보자.

- H/W
    - CPU: Intel Core i7 7700
    - Memory: DDR4 8 GB
    - GPU: NVIDIA GeForece GTX 1050
- S/W
    - Windows 10
    - CUDA: 11.2
    - cuDNN: 8.4.0
    - Python: 3.8.8
    

```{python, eval = FALSE}
import tensorflow as tf
import numpy as np                                
import matplotlib.pyplot as plt
import keras as k
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from keras.optimizers import SGD, Adam
from keras.models import load_model
from keras import backend as K

import tensorflow as tf
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.compat.v1.Session(config = config)

#data preprocessing
(x_train, y_train), (x_test, y_test) = mnist.load_data()
img_rows, img_cols = 28,28
x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
input_shape = (img_rows, img_cols, 1)
x_test=x_test.astype('float32')
x_train=x_train.astype('float32')
mean=np.mean(x_train)
std=np.std(x_train)
x_test = (x_test-mean)/std
x_train = (x_train-mean)/std

#labels
num_classes=10
y_train = k.utils.to_categorical(y_train, num_classes)
y_test = k.utils.to_categorical(y_test, num_classes)
```


## GPU 훈련

```{python, eval = FALSE}

#build model

num_filter=32
num_dense=512
drop_dense=0.7
ac='relu'
learningrate=0.001

model = Sequential()

model.add(Conv2D(num_filter, (3, 3), activation=ac, input_shape=(28, 28, 1),padding='same'))
model.add(BatchNormalization(axis=-1))
model.add(Conv2D(num_filter, (3, 3), activation=ac,padding='same'))
model.add(BatchNormalization(axis=-1))
model.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 14x14x32

model.add(Conv2D(2*num_filter, (3, 3), activation=ac,padding='same'))
model.add(BatchNormalization(axis=-1))
model.add(Conv2D(2*num_filter, (3, 3), activation=ac,padding='same'))
model.add(BatchNormalization(axis=-1))
model.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 7x7x64 = 3136 neurons

model.add(Flatten())                        
model.add(Dense(num_dense, activation=ac))
model.add(BatchNormalization())
model.add(Dropout(drop_dense))
model.add(Dense(10, activation='softmax'))

adm=Adam(learning_rate=learningrate, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=adm)

# Allocator (GPU_0_bfc) ran out of memory trying to allocate 오류
# gpus = tf.config.experimental.list_physical_devices('GPU')
# if gpus:
#     # Restrict TensorFlow to only allocate 1GB of memory on the first GPU
#     try:
#         tf.config.experimental.set_virtual_device_configuration(gpus[0],
#        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])
#         logical_gpus = tf.config.experimental.list_logical_devices('GPU')
#         print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
#     except RuntimeError as e:
#         # Virtual devices must be set before GPUs have been initialized
#         print(e)

# with GPU (the default in my setup)
for i in range(7):
    k=8*2**i
    print("batch size "+str(k))
    model.fit(x_train, y_train, batch_size=k, epochs=1, validation_data=(x_test, y_test))

```


## CPU 훈련 


```{python, eval = FALSE}
# with CPU only: (tensorflow tries to use all cores available)

with tf.device("/cpu:0"):
    model = Sequential()

    model.add(Conv2D(num_filter, (3, 3), activation=ac, input_shape=(28, 28, 1),padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(Conv2D(num_filter, (3, 3), activation=ac,padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 14x14x32

    model.add(Conv2D(2*num_filter, (3, 3), activation=ac,padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(Conv2D(2*num_filter, (3, 3), activation=ac,padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 7x7x64 = 3136 neurons

    model.add(Flatten())                        
    model.add(Dense(num_dense, activation=ac))
    model.add(BatchNormalization())
    model.add(Dropout(drop_dense))
    model.add(Dense(10, activation='softmax'))

    adm=Adam(learning_rate=learningrate, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=adm)
    
    
with tf.device("/cpu:0"):
    for i in range(1):
        k=8*2**i
        print("batch size "+str(k))
        model.fit(x_train, y_train, batch_size=k, epochs=1, validation_data=(x_test, y_test))


```


```
...
batch size 512

  1/118 [..............................] - ETA: 2:16 - loss: 0.0022 - accuracy: 1.0000
  2/118 [..............................] - ETA: 2:24 - loss: 0.0080 - accuracy: 0.9980
  3/118 [..............................] - ETA: 2:15 - loss: 0.0118 - accuracy: 0.9967
  4/118 [>.............................] - ETA: 2:15 - loss: 0.0094 - accuracy: 0.9976
  5/118 [>.............................] - ETA: 2:13 - loss: 0.0112 - accuracy: 0.9969
  6/118 [>.............................] - ETA: 2:12 - loss: 0.0105 - accuracy: 0.9967
  7/118 [>.............................] - ETA: 2:11 - loss: 0.0129 - accuracy: 0.9955
  8/118 [=>............................] - ETA: 2:09 - loss: 0.0122 - accuracy: 0.9958
  9/118 [=>............................] - ETA: 2:08 - loss: 0.0114 - accuracy: 0.9961
 10/118 [=>............................] - ETA: 2:06 - loss: 0.0116 - accuracy: 0.9961
 11/118 [=>............................] - ETA: 2:05 - loss: 0.0124 - accuracy: 0.9959
 12/118 [==>...........................] - ETA: 2:03 - loss: 0.0117 - accuracy: 0.9963
 13/118 [==>...........................] - ETA: 2:02 - loss: 0.0122 - accuracy: 0.9959
 14/118 [==>...........................] - ETA: 2:01 - loss: 0.0116 - accuracy: 0.9962
 15/118 [==>...........................] - ETA: 2:00 - loss: 0.0111 - accuracy: 0.9964
 16/118 [===>..........................] - ETA: 2:01 - loss: 0.0108 - accuracy: 0.9965
 17/118 [===>..........................] - ETA: 2:00 - loss: 0.0106 - accuracy: 0.9966
 18/118 [===>..........................] - ETA: 1:59 - loss: 0.0108 - accuracy: 0.9965
 19/118 [===>..........................] - ETA: 1:58 - loss: 0.0105 - accuracy: 0.9966
 20/118 [====>.........................] - ETA: 1:57 - loss: 0.0103 - accuracy: 0.9966
 21/118 [====>.........................] - ETA: 1:55 - loss: 0.0099 - accuracy: 0.9967
 22/118 [====>.........................] - ETA: 1:54 - loss: 0.0097 - accuracy: 0.9968
 23/118 [====>.........................] - ETA: 1:53 - loss: 0.0096 - accuracy: 0.9969
 24/118 [=====>........................] - ETA: 1:52 - loss: 0.0097 - accuracy: 0.9967
 25/118 [=====>........................] - ETA: 1:51 - loss: 0.0099 - accuracy: 0.9966
 26/118 [=====>........................] - ETA: 1:51 - loss: 0.0109 - accuracy: 0.9966
 27/118 [=====>........................] - ETA: 1:49 - loss: 0.0108 - accuracy: 0.9966
 28/118 [======>.......................] - ETA: 1:48 - loss: 0.0104 - accuracy: 0.9967
 29/118 [======>.......................] - ETA: 1:47 - loss: 0.0107 - accuracy: 0.9966
 30/118 [======>.......................] - ETA: 1:46 - loss: 0.0105 - accuracy: 0.9966
 31/118 [======>.......................] - ETA: 1:45 - loss: 0.0106 - accuracy: 0.9966
 32/118 [=======>......................] - ETA: 1:43 - loss: 0.0107 - accuracy: 0.9966
 33/118 [=======>......................] - ETA: 1:42 - loss: 0.0108 - accuracy: 0.9966
 34/118 [=======>......................] - ETA: 1:41 - loss: 0.0106 - accuracy: 0.9967
 35/118 [=======>......................] - ETA: 1:40 - loss: 0.0110 - accuracy: 0.9964
 36/118 [========>.....................] - ETA: 1:39 - loss: 0.0110 - accuracy: 0.9964
 37/118 [========>.....................] - ETA: 1:37 - loss: 0.0108 - accuracy: 0.9965
 38/118 [========>.....................] - ETA: 1:36 - loss: 0.0105 - accuracy: 0.9966
 39/118 [========>.....................] - ETA: 1:35 - loss: 0.0104 - accuracy: 0.9967
 40/118 [=========>....................] - ETA: 1:34 - loss: 0.0105 - accuracy: 0.9967
 41/118 [=========>....................] - ETA: 1:32 - loss: 0.0103 - accuracy: 0.9968
 42/118 [=========>....................] - ETA: 1:31 - loss: 0.0104 - accuracy: 0.9968
 43/118 [=========>....................] - ETA: 1:30 - loss: 0.0102 - accuracy: 0.9968
 44/118 [==========>...................] - ETA: 1:29 - loss: 0.0102 - accuracy: 0.9968
 45/118 [==========>...................] - ETA: 1:28 - loss: 0.0101 - accuracy: 0.9969
 46/118 [==========>...................] - ETA: 1:27 - loss: 0.0099 - accuracy: 0.9969
 47/118 [==========>...................] - ETA: 1:26 - loss: 0.0099 - accuracy: 0.9969
 48/118 [===========>..................] - ETA: 1:24 - loss: 0.0101 - accuracy: 0.9969
 49/118 [===========>..................] - ETA: 1:23 - loss: 0.0099 - accuracy: 0.9969
 50/118 [===========>..................] - ETA: 1:22 - loss: 0.0101 - accuracy: 0.9967
 51/118 [===========>..................] - ETA: 1:21 - loss: 0.0102 - accuracy: 0.9966
 52/118 [============>.................] - ETA: 1:19 - loss: 0.0101 - accuracy: 0.9967
 53/118 [============>.................] - ETA: 1:18 - loss: 0.0101 - accuracy: 0.9967
 54/118 [============>.................] - ETA: 1:17 - loss: 0.0101 - accuracy: 0.9967
 55/118 [============>.................] - ETA: 1:16 - loss: 0.0102 - accuracy: 0.9967
 56/118 [=============>................] - ETA: 1:15 - loss: 0.0101 - accuracy: 0.9967
 57/118 [=============>................] - ETA: 1:13 - loss: 0.0100 - accuracy: 0.9967
 58/118 [=============>................] - ETA: 1:12 - loss: 0.0100 - accuracy: 0.9968
 59/118 [==============>...............] - ETA: 1:11 - loss: 0.0099 - accuracy: 0.9968
 60/118 [==============>...............] - ETA: 1:10 - loss: 0.0099 - accuracy: 0.9968
 61/118 [==============>...............] - ETA: 1:08 - loss: 0.0100 - accuracy: 0.9967
 62/118 [==============>...............] - ETA: 1:07 - loss: 0.0099 - accuracy: 0.9968
 63/118 [===============>..............] - ETA: 1:06 - loss: 0.0099 - accuracy: 0.9967
 64/118 [===============>..............] - ETA: 1:05 - loss: 0.0101 - accuracy: 0.9967
 65/118 [===============>..............] - ETA: 1:04 - loss: 0.0100 - accuracy: 0.9968
 66/118 [===============>..............] - ETA: 1:02 - loss: 0.0099 - accuracy: 0.9967
 67/118 [================>.............] - ETA: 1:01 - loss: 0.0098 - accuracy: 0.9968
 68/118 [================>.............] - ETA: 1:00 - loss: 0.0099 - accuracy: 0.9968
 69/118 [================>.............] - ETA: 59s - loss: 0.0098 - accuracy: 0.9968 
 70/118 [================>.............] - ETA: 58s - loss: 0.0097 - accuracy: 0.9968
 71/118 [=================>............] - ETA: 57s - loss: 0.0096 - accuracy: 0.9968
 72/118 [=================>............] - ETA: 55s - loss: 0.0095 - accuracy: 0.9969
 73/118 [=================>............] - ETA: 54s - loss: 0.0095 - accuracy: 0.9969
 74/118 [=================>............] - ETA: 53s - loss: 0.0095 - accuracy: 0.9969
 75/118 [==================>...........] - ETA: 52s - loss: 0.0095 - accuracy: 0.9968
 76/118 [==================>...........] - ETA: 50s - loss: 0.0096 - accuracy: 0.9969
 77/118 [==================>...........] - ETA: 49s - loss: 0.0096 - accuracy: 0.9969
 78/118 [==================>...........] - ETA: 48s - loss: 0.0095 - accuracy: 0.9969
 79/118 [===================>..........] - ETA: 47s - loss: 0.0094 - accuracy: 0.9969
 80/118 [===================>..........] - ETA: 46s - loss: 0.0094 - accuracy: 0.9969
 81/118 [===================>..........] - ETA: 45s - loss: 0.0093 - accuracy: 0.9969
 82/118 [===================>..........] - ETA: 43s - loss: 0.0093 - accuracy: 0.9970
 83/118 [====================>.........] - ETA: 42s - loss: 0.0093 - accuracy: 0.9969
 84/118 [====================>.........] - ETA: 41s - loss: 0.0093 - accuracy: 0.9970
 85/118 [====================>.........] - ETA: 40s - loss: 0.0093 - accuracy: 0.9969
 86/118 [====================>.........] - ETA: 38s - loss: 0.0093 - accuracy: 0.9969
 87/118 [=====================>........] - ETA: 37s - loss: 0.0092 - accuracy: 0.9970
 88/118 [=====================>........] - ETA: 36s - loss: 0.0092 - accuracy: 0.9970
 89/118 [=====================>........] - ETA: 35s - loss: 0.0091 - accuracy: 0.9970
 90/118 [=====================>........] - ETA: 34s - loss: 0.0093 - accuracy: 0.9970
 91/118 [======================>.......] - ETA: 32s - loss: 0.0095 - accuracy: 0.9969
 92/118 [======================>.......] - ETA: 31s - loss: 0.0094 - accuracy: 0.9969
 93/118 [======================>.......] - ETA: 30s - loss: 0.0094 - accuracy: 0.9970
 94/118 [======================>.......] - ETA: 29s - loss: 0.0093 - accuracy: 0.9970
 95/118 [=======================>......] - ETA: 28s - loss: 0.0093 - accuracy: 0.9970
 96/118 [=======================>......] - ETA: 26s - loss: 0.0092 - accuracy: 0.9970
 97/118 [=======================>......] - ETA: 25s - loss: 0.0092 - accuracy: 0.9970
 98/118 [=======================>......] - ETA: 24s - loss: 0.0092 - accuracy: 0.9970
 99/118 [========================>.....] - ETA: 23s - loss: 0.0091 - accuracy: 0.9970
100/118 [========================>.....] - ETA: 21s - loss: 0.0091 - accuracy: 0.9971
101/118 [========================>.....] - ETA: 20s - loss: 0.0090 - accuracy: 0.9971
102/118 [========================>.....] - ETA: 19s - loss: 0.0091 - accuracy: 0.9971
103/118 [=========================>....] - ETA: 18s - loss: 0.0091 - accuracy: 0.9971
104/118 [=========================>....] - ETA: 17s - loss: 0.0092 - accuracy: 0.9970
105/118 [=========================>....] - ETA: 15s - loss: 0.0091 - accuracy: 0.9971
106/118 [=========================>....] - ETA: 14s - loss: 0.0092 - accuracy: 0.9971
107/118 [==========================>...] - ETA: 13s - loss: 0.0091 - accuracy: 0.9971
108/118 [==========================>...] - ETA: 12s - loss: 0.0091 - accuracy: 0.9971
109/118 [==========================>...] - ETA: 10s - loss: 0.0090 - accuracy: 0.9971
110/118 [==========================>...] - ETA: 9s - loss: 0.0091 - accuracy: 0.9971 
111/118 [===========================>..] - ETA: 8s - loss: 0.0090 - accuracy: 0.9971
112/118 [===========================>..] - ETA: 7s - loss: 0.0091 - accuracy: 0.9971
113/118 [===========================>..] - ETA: 6s - loss: 0.0091 - accuracy: 0.9971
114/118 [===========================>..] - ETA: 4s - loss: 0.0090 - accuracy: 0.9971
115/118 [============================>.] - ETA: 3s - loss: 0.0090 - accuracy: 0.9971
116/118 [============================>.] - ETA: 2s - loss: 0.0090 - accuracy: 0.9971
117/118 [============================>.] - ETA: 1s - loss: 0.0090 - accuracy: 0.9971
118/118 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9971
118/118 [==============================] - 147s 1s/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.0149 - val_accuracy: 0.9944
<keras.callbacks.History object at 0x000001535024DD30>
```

