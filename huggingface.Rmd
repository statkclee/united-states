---
layout: page
title: "20ë¶„ë§Œì— ëë‚´ëŠ” ë”¥ëŸ¬ë‹"
subtitle: "í—ˆê¹…í˜ì´ìŠ¤"
author:
  name: "[í•œêµ­ R ì‚¬ìš©ìíšŒ](https://r2bit.com/)"
output:
  html_document: 
    theme:
      version: 4
    toc: yes
    toc_depth: 2
    toc_float: true
    highlight: tango
    code_folding: show
    number_section: true
    self_contained: true
urlcolor: blue
linkcolor: bluee
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: sentence
header-includes: 
  - \usepackage{tikz}
  - \usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
---

```{r setup, include=FALSE}
# source("tools/chunk-options.R")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,
                    comment="", digits = 3, tidy = FALSE, prompt = FALSE, fig.align = 'center')

library(tidyverse)
```

![](assets/cloud/huggingface-cache.png)

# ë°ì´í„°ì…‹

[Hugging Face](https://huggingface.co/) ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„°ì™€ ì‚¬ì „í•™ìŠµëª¨ë¸ì„
ë‹¤ìš´ë¡œë“œ ë°›ì•„ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ê°œë°œ ìƒì‚°ì„±ì„ ë†’ì¼ ìˆ˜ ìˆë‹¤.

[Hugging Face](https://huggingface.co/)ì—ì„œ ë‹¤ë¥¸ ì„¤ì •ì„ íŠ¹ë³„íˆ í•˜ì§€ ì•Šê²Œ ë˜ë©´ 
ìœˆë„ìš°ì˜ ê²½ìš° ë‹¤ìŒ ë””ë ‰í† ë¦¬ì— ë°ì´í„°ì™€ ì‚¬ì „í•™ìŠµëª¨ë¸ì´ ì €ì¥ëœë‹¤.

- `C:\Users\ì‚¬ìš©ìëª…\.cache\huggingface\datasets\`

`datasets` ì— í¬í•¨ëœ ë°ì´í…Ÿì„ íŒŒì´ì¬ì—ì„œ ì¼ë³„í•œë‹¤. 

```{python configure-download, eval = TRUE}
## ë‹¤ìš´ë¡œë“œ ë°›ì€ ë°ì´í„°ì…‹ì„ ì‚­ì œí•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥
from datasets import disable_caching
from datasets import load_dataset
disable_caching()

from datasets import list_datasets
datasets_list = list_datasets()
len(datasets_list)

```

ê¸°ê³„í•™ìŠµì˜ ê°€ì¥ ëŒ€í‘œì ì¸ ë°ì´í„°ì…‹ì´ ë¶“ê½ƒ(iris) ë°ì´í„°ì…‹ì´ ìœ ëª…í•˜ê¸° ë•Œë¬¸ì— 
`datasets` íŒ¨í‚¤ì§€ì— í¬í•¨ëœ ë°ì´í„° ì¤‘ iris ë°ì´í„°ì„¸ì„ ë‹¤ìš´ë¡œë“œë°›ì•„ ì´ë¥¼ 
ê¸°ê³„í•™ìŠµ ë¶„ì„ì— í™œìš©í•œë‹¤.

`reticulate` íŒ¨í‚¤ì§€ íŒŒì´ì¬ ê°ì²´ë¥¼ Rì—ì„œ ê°€ì ¸ì™€ì„œ `iris` ë°ì´í„°ì…‹ì„ í™•ì¸í•œë‹¤.


```{r}
library(reticulate)
library(tidyverse)

py$datasets_list %>% 
  enframe() %>% 
  filter(str_detect(value, "iris"))
```

ë¶“ê½ƒ ë°ì´í„°ë¥¼ íŒ¬ë‹¤ìŠ¤ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•œ í›„ì— ê¸°ê³„í•™ìŠµì„ ìœ„í•œ ì¤€ë¹„ ë°ì´í„°ë¡œ ë˜ì—ˆëŠ”ì§€ ì¶œë ¥í•˜ì—¬ í™•ì¸í•œë‹¤.

```{python, eval = FALSE}
iris = load_dataset('scikit-learn/iris', 
                       download_mode="reuse_dataset_if_exists",
                       cache_dir='z:\dataset')

print(iris)

iris_pd = iris['train'].to_pandas()

iris_pd
```

```
      Id  SepalLengthCm  ...  PetalWidthCm         Species
0      1            5.1  ...           0.2     Iris-setosa
1      2            4.9  ...           0.2     Iris-setosa
2      3            4.7  ...           0.2     Iris-setosa
3      4            4.6  ...           0.2     Iris-setosa
4      5            5.0  ...           0.2     Iris-setosa
..   ...            ...  ...           ...             ...
145  146            6.7  ...           2.3  Iris-virginica
146  147            6.3  ...           1.9  Iris-virginica
147  148            6.5  ...           2.0  Iris-virginica
148  149            6.2  ...           2.3  Iris-virginica
149  150            5.9  ...           1.8  Iris-virginica

[150 rows x 6 columns]
```


ë§ˆì°¬ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹(squad), ì˜í™”í‰ì  `imdb` ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•œë‹¤.
ì €ì¥ì¥ì†Œë¡œ `cache_dir='z:\dataset'` ì™€ ê°™ì´ (ì‹œë†€ë¡œì§€) NAS ë””ë ‰í† ë¦¬ë¥¼ ì§€ì •í•˜ì—¬ ì €ì¥í•œë‹¤.

```{python, eval = FALSE}
squad = load_dataset('squad', 
                       download_mode="reuse_dataset_if_exists",
                       cache_dir='z:\dataset')
                       
imdb = load_dataset('imdb', 
                       download_mode="reuse_dataset_if_exists",
                       cache_dir='z:\dataset')
```


# ì´ë¯¸ì§€ ë°ì´í„°ì…‹

ê°œì™€ ê³ ì–‘ì´ ì´ë¯¸ì§€ ë¶„ë¥˜ê°€ ì²« ë”¥ëŸ¬ë‹ ëª¨í˜•ì´ë¼ ì´ë¥¼ ì˜ êµ¬ë¶„í•˜ëŠ” ë¶„ë¥˜ê¸°ë¥¼ ì œì‘í•˜ê¸° ìœ„í•´ì„œ 
ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•œë‹¤. ë¨¼ì € ê³ ì–‘ì´(`cats`)ê°€ í¬í•¨ëœ ë°ì´í„°ì…‹ë¥¼ ì°¾ì•„ ... `cats_vs_dogs` ëª…ì¹­ì„ ê°–ëŠ” ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ í•œë‹¤.

```{r}
py$datasets_list %>% 
  enframe() %>% 
  filter(str_detect(value, "cats"))
```

ëŒ€ì•ˆìœ¼ë¡œ [Hugging Face - Datasets](https://huggingface.co/datasets) ì—ì„œ ê²€ìƒ‰ì„ ë„£ì–´ 
ê°€ì¥ ë§ì€ ë‹¤ìš´ë¡œë“œë¥¼ ê¸°ë¡í•œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•œë‹¤.


```{python, eval = FALSE}
cats_vs_dogs = load_dataset('hf-internal-testing/cats_vs_dogs_sample', 
                       download_mode="reuse_dataset_if_exists",
                       cache_dir='z:\dataset')

cats_vs_dogs['train'].to_pandas()
```

```
                                                image  labels
0   {'bytes': None, 'path': 'z:\dataset\downloads\...       0
1   {'bytes': None, 'path': 'z:\dataset\downloads\...       0
2   {'bytes': None, 'path': 'z:\dataset\downloads\...       0
3   {'bytes': None, 'path': 'z:\dataset\downloads\...       0
4   {'bytes': None, 'path': 'z:\dataset\downloads\...       0
5   {'bytes': None, 'path': 'z:\dataset\downloads\...       0
6   {'bytes': None, 'path': 'z:\dataset\downloads\...       0
7   {'bytes': None, 'path': 'z:\dataset\downloads\...       0
8   {'bytes': None, 'path': 'z:\dataset\downloads\...       0
9   {'bytes': None, 'path': 'z:\dataset\downloads\...       0
10  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
11  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
12  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
13  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
14  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
15  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
16  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
17  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
18  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
19  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
20  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
21  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
22  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
23  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
24  {'bytes': None, 'path': 'z:\dataset\downloads\...       0
25  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
26  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
27  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
28  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
29  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
30  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
31  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
32  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
33  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
34  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
35  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
36  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
37  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
38  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
39  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
40  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
41  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
42  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
43  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
44  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
45  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
46  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
47  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
48  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
49  {'bytes': None, 'path': 'z:\dataset\downloads\...       1
```

ë”¥ëŸ¬ë‹ ê°€ì¥ ì¸ê¸°ê°€ ë§ì€ `mnist` ë°ì´í„°ì…‹ë„ ë‹¤ìš´ë¡œë“œ ë°›ì•„ NAS ì €ì¥ì†Œì— ì €ì¥í•œë‹¤.
ê³µí†µì ìœ¼ë¡œ `.arrow` íŒŒì¼ í™•ì¥ìê°€ë¥¼ ê°–ëŠ” íŒŒì¼ë¡œ ì €ì¥ëœë‹¤.

```{python, eval = FALSE}
mnist = load_dataset('mnist', 
                       download_mode="reuse_dataset_if_exists",
                       cache_dir='z:\dataset')

mnist
```

```
DatasetDict({
    train: Dataset({
        features: ['image', 'label'],
        num_rows: 60000
    })
    test: Dataset({
        features: ['image', 'label'],
        num_rows: 10000
    })
})
```


# HF íŒŒì´í”„ë¼ì¸

## ëª¨í˜•/ë°ì´í„° ì €ì¥ìœ„ì¹˜

í—ˆê¹…í˜ì´ìŠ¤ ìºì‰¬ ì €ì¥ì†Œë¥¼ ì§€ì •í•˜ëŠ” ë°©ì‹ì€ í¬ê²Œ 3ê°€ì§€ë¡œ ë‚˜ëˆ ì§„ë‹¤.

- íŒŒì´ì¬ ë‚´ í™˜ê²½ì„¤ì •

```
import os
os.environ['TRANSFORMERS_CACHE'] = '/blabla/cache/'
```

- ë°°ì‰¬(bash) í™˜ê²½ì„¤ì •
    - `export TRANSFORMERS_CACHE=/blabla/cache/`
- ë”¥ëŸ¬ë‹ ì½”ë“œì—ì„œ ì§ì ‘ ì§€ì •

```
tokenizer = AutoTokenizer.from_pretrained("roberta-base", cache_dir="new_cache_dir/")

model = AutoModelForMaskedLM.from_pretrained("roberta-base", cache_dir="new_cache_dir/")
```

## NLP ê¸°ë³¸ê¸°


[Hugging Face Course](https://www.youtube.com/playlist?list=PLo2EIpI_JMQtNtKNFFSMNIZwspj8H7-sQ)ì— ë‹´ê¸° ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìì„¸íˆ ì‚´í´ë³´ì.

### ê°ì„±ë¶„ì„

`distilbert-base-uncased-finetuned-sst-2-english` í•™ìŠµëœ ëª¨í˜•ìœ¼ë¡œ ë¬¸ì¥ì— 
ë‹´ê¸´ ê°ì„±ì„ ë¶„ì„í•œë‹¤. í—ˆê¹…í˜ì´ìŠ¤ ìºì‰¬ ë””ë ‰í† ë¦¬ì— ì €ì¥ëœ 'z:/dataset/hf' ì—ì„œ
ëª¨í˜•ì„ ê°€ì ¸ì™€ì„œ ê°ì„±ë¶„ì„ì„ ì§„í–‰í•œë‹¤.

```{python}
# ! pip install datasets transformers[sentencepiece]

import os
os.environ['TRANSFORMERS_CACHE'] = 'z:/dataset/hf'

from transformers import pipeline

classifier = pipeline("sentiment-analysis")
                      
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
        "We are very happy to show you the ğŸ¤— Transformers library.", 
        "We hope you don't hate it."
    ]
)
```

í•œë‹¨ê³„ ë” ë“¤ì–´ê°€ tokenizerì™€ modelì„ ê°ê¸° ë‹¬ë¦¬í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•œ í›„ì— 
ë¬¸ì¥ì— ëŒ€í•œ ê°ì„±ì„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.

![](assets/dl_2/full_nlp_pipeline.svg)

```{python, eval = FALSE}
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir="z:/dataset/hf")

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name,
           cache_dir="z:/dataset/hf")

classifier = pipeline("sentiment-analysis",
                       model = pt_model,
                       tokenizer = tokenizer)
                      
classifier(
    [
        "ì§€ê¸ˆ ê¸°ë¶„ì´ ì¢‹ìŠµë‹ˆë‹¤.",
        "ë‚˜ëŠ” í–‰ë³µí•©ë‹ˆë‹¤",
        "ë¬´ì²™ì´ë‚˜ ìŠ¬í”„ê³  ì„œëŸ½ìŠµë‹ˆë‹¤",
        "ì˜í™”ê°€ ê·¸ì €ê·¸ë ‡ë‹¤."
    ]
)
 
# [{'label': '4 stars', 'score': 0.3802778422832489}, {'label': '5 stars', 'score': 0.5900374054908752}, {'label': '1 star', 'score': 0.5471497774124146}, {'label': '3 stars', 'score': 0.3373289406299591}]

```


### ì£¼ì œ ë¶„ë¥˜

ì£¼ì–´ì§„ ë¬¸ì¥ì´ ì–´ë–¤ ì£¼ì œë¡œ ë¶„ë¥˜ë˜ëŠ”ì§€ë¥¼ `facebook/bart-large-mnli` ì‚¬ì „í•™ìŠµëª¨í˜•ìœ¼ë¡œ
ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤. [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)ì—ì„œ ìì„¸í•œ ì‚¬í•­ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```{python, eval = FALSE}
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import pipeline

nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli',
            cache_dir="z:/dataset/hf")
            
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli',
            cache_dir="z:/dataset/hf")

classifier = pipeline('zero-shot-classification',
                       model     = nli_model,
                       tokenizer = tokenizer)
                       
sequence_to_classify = "one day I will see the world"
candidate_labels = ['travel', 'cooking', 'dancing']
classifier(sequence_to_classify, candidate_labels,  multi_label=True)
# {'sequence': 'one day I will see the world', 'labels': ['travel', 'dancing', 'cooking'], 'scores': [0.994511067867279, 0.005706145893782377, 0.0018192846328020096]}
```

### í…ìŠ¤íŠ¸ ìƒì„±

[gpt2](https://huggingface.co/gpt2)ë¥¼ ì‚¬ìš©í•´ì„œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

```{python, eval = FALSE}
from transformers import pipeline, set_seed
from transformers import GPT2Tokenizer, GPT2LMHeadModel

gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir="z:/dataset/hf")
gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2', cache_dir="z:/dataset/hf")

generator = pipeline('text-generation', 
                      tokenizer = gpt2_tokenizer,
                      model     = gpt2_model)
set_seed(777)
generator("The White man worked as a", max_length=12, num_return_sequences=3)
# [{'generated_text': 'The White man worked as a janitor, a regular office'}, {'generated_text': 'The White man worked as a sales rep for the paper.'}, {'generated_text': 'The White man worked as a contractor for Waffle House in'}]

generator("In this course, we will teach you how to", max_length=20, num_return_sequences=3)

# [{'generated_text': 'In this course, we will teach you how to create a reusable, self-sustaining app'}, {'generated_text': 'In this course, we will teach you how to create a perfect web application.'}, {'generated_text': 'In this course, we will teach you how to use both the HN and the NN channels'}]
```


### ë¹ˆì¹¸ ì±„ì›Œë„£ê¸°

`BertForMaskedLM` ëª¨í˜•ì„ ì‚¬ìš©í•´ì„œ `[MASK]` ë¹ˆì¹¸ ì±„ì›Œë„£ê¸°ë¥¼ í•  ìˆ˜ ìˆë‹¤.

```{python, eval = FALSE}
from transformers import pipeline, set_seed
from transformers import  BertTokenizer, BertForMaskedLM

fill_mask_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', cache_dir="z:/dataset/hf/mask")
fill_mask_model = BertForMaskedLM.from_pretrained("bert-base-uncased", cache_dir="z:/dataset/hf/mask")

unmasker = pipeline('fill-mask', 
                     tokenizer = fill_mask_tokenizer,
                     model = fill_mask_model)
                     
unmasker("The White man worked as a [MASK].", top_k = 3)
# [{'score': 0.16597844660282135, 'token': 10533, 'token_str': 'c a r p e n t e r', 'sequence': 'the white man worked as a carpenter.'}, {'score': 0.09424988180398941, 'token': 7500, 'token_str': 'f a r m e r', 'sequence': 'the white man worked as a farmer.'}, {'score': 0.07112522423267365, 'token': 20987, 'token_str': 'b l a c k s m i t h', 'sequence': 'the white man worked as a blacksmith.'}]

unmasker("The black woman worked as a [MASK].", top_k = 3)
# [{'score': 0.21546946465969086, 'token': 6821, 'token_str': 'n u r s e', 'sequence': 'the black woman worked as a nurse.'}, {'score': 0.19593699276447296, 'token': 13877, 'token_str': 'w a i t r e s s', 'sequence': 'the black woman worked as a waitress.'}, {'score': 0.09739767760038376, 'token': 10850, 'token_str': 'm a i d', 'sequence': 'the black woman worked as a maid.'}]

unmasker("The Asian worked as a [MASK].", top_k = 3)
# [{'score': 0.12938520312309265, 'token': 7500, 'token_str': 'f a r m e r', 'sequence': 'the asian worked as a farmer.'}, {'score': 0.09072186052799225, 'token': 3836, 'token_str': 't e a c h e r', 'sequence': 'the asian worked as a teacher.'}, {'score': 0.039076220244169235, 'token': 10533, 'token_str': 'c a r p e n t e r', 'sequence': 'the asian worked as a carpenter.'}]

unmasker("The Korean worked as a [MASK].", top_k = 3)
# [{'score': 0.11826611310243607, 'token': 7500, 'token_str': 'f a r m e r', 'sequence': 'the korean worked as a farmer.'}, {'score': 0.09532739222049713, 'token': 3836, 'token_str': 't e a c h e r', 'sequence': 'the korean worked as a teacher.'}, {'score': 0.04381617531180382, 'token': 5160, 'token_str': 'l a w y e r', 'sequence': 'the korean worked as a lawyer.'}]
```


### ì§ˆì˜ì‘ë‹µ

`SQuAD2.0` ë°ì´í„°ì…‹ì— ê¸°ë°˜í•œ 
[deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2) ëª¨í˜•ì„ ë°”íƒ•ìœ¼ë¡œ 
ì§ˆì˜ì‘ë‹µì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.

```{python, eval = FALSE}
from transformers import pipeline
from transformers import AutoModelForQuestionAnswering, AutoTokenizer

qna_tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2",
                cache_dir="z:/dataset/pretrained/")

qna_model = AutoModelForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2",              
            cache_dir="z:/dataset/pretrained/")

qNa = pipeline('question-answering', 
                model=qna_model,
                tokenizer=qna_tokenizer) 


paragraph = '''
A new study estimates that if the US had universally mandated masks on 1 April, there could have been nearly 40% fewer deaths by the start of June. Containment policies had a large impact on the number of COVID-19 cases and deaths, directly by reducing transmission rates and indirectly by constraining peopleâ€™s behaviour. They account for roughly half the observed change in the growth rates of cases and deaths.
'''

qNa({'question': 'Which country is this article about?',
     'context': f'{paragraph}'})
# {'score': 0.019898569211363792, 'start': 35, 'end': 37, 'answer': 'US'}
qNa({'question': 'Which disease is discussed in this article?',
     'context': f'{paragraph}'})
# {'score': 0.00024747499264776707, 'start': 206, 'end': 214, 'answer': 'COVID-19'}
```

### ë²ˆì—­

[Helsinki-NLP/opus-mt-ko-en](https://huggingface.co/Helsinki-NLP/opus-mt-ko-en) ëª¨ë¸ì€
í•œê¸€ì„ ì…ë ¥ë°›ì•„ ì˜ì–´ë¡œ ë²ˆì—­í•œë‹¤.

```{python, eval = FALSE}
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

translator_tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-ko-en",
                cache_dir="z:/dataset/hf/")

translator_model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-ko-en",              
            cache_dir="z:/dataset/hf/")


translator = pipeline("translation", 
                      tokenizer = translator_tokenizer,
                      model =  translator_model)
                      
translator("ëŒ€í•œë¯¼êµ­ ì¶•êµ¬ê°€ ë“œë””ì–´ ë…ì¼ì„ ê²©íŒŒí–ˆìŠµë‹ˆë‹¤.")
# [{'translation_text': 'South Korean football finally destroyed Germany.'}]
translator("President Yoon did a great work in Korean history")
# [{'translation_text': 'President Yoon did a great job in Coran history'}]
```

### í…ìŠ¤íŠ¸ ìš”ì•½

CNN Daily Mail ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³  ìˆëŠ”
[facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn) ëª¨ë¸ì„
í™œìš©í•˜ì—¬ ë¬¸ë‹¨ì„ ìš”ì•½í•œë‹¤.

```{python, eval = FALSE}
from transformers import pipeline
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

summary_tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-cnn",
                                                  cache_dir="z:/dataset/hf/")
                                          
summary_model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-large-cnn",
                                          cache_dir="z:/dataset/hf/")


summarizer = pipeline("summarization", 
                      model = summary_model,
                      tokenizer = summary_tokenizer)

paragraph = '''
A new study estimates that if the US had universally mandated masks on 1 April, there could have been nearly 40% fewer deaths by the start of June. Containment policies had a large impact on the number of COVID-19 cases and deaths, directly by reducing transmission rates and indirectly by constraining peopleâ€™s behaviour. They account for roughly half the observed change in the growth rates of cases and deaths.
'''
                      
summarizer(paragraph, max_length=50, min_length=30, do_sample=False)

```

```
[{'summary_text': 'Study estimates that if the US had universally mandated masks on 1 April, there could have been nearly 40% fewer deaths by the start of June. Containment policies had a large impact on the number of COVID-19 cases and deaths'}]
```
